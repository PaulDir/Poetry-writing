# Poetry-writing
自然语言处理：写诗机器人
## 基于LSTM的写诗机器人

### 1.	选题动机
   中华民族悠久的历史孕育了绵长的诗歌文化，而唐诗则是诗歌文化中的一座高峰。古人通过诗歌抒发情感，而通过先人们留下的诗歌，现代人也得以窥见当时人们给生活。我一直以来对于诗歌，尤其是唐诗有着浓厚的兴趣，曾经也模仿过古代的大家创作了一些诗。由于能力有限，我写出来的自然都只是一些不入流的作品，不过，对于此我也并不感到沮丧，毕竟写诗只是我的一个兴趣，并没有花费太多精力在上面，我一直觉得，要是我从小就像学习语文数学那样学写诗，说不定我也能成为大师。  
本学期我接触到了自然语言处理，得知机器能在短时间内针对大量已有知识进行学习，也就是说，我所做不到的（花费大量时间对已有诗歌集进行学习模仿）可以借助机器来完成。我自己创造不出那些华丽的诗歌，但也许机器可以替我做到，为此，我准备通过LSTM实现一个能自动写诗的程序。  

###  2.	数据获取过程
  诗歌的种类有很多，我比较喜欢唐诗，本次实验所选取数据集便是全唐诗，这个数据集来源于https://github.com/chinese-poetry/chinese-poetry/dataset。
数据集格式如图2.1：题目、作者名、诗的内容通过“：：”分隔开来。  
 ![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/1.png)
                              
   在使用数据之前，还需要对数据集进行一些处理。首先，需要机器学习的部分只有诗的内容，因此要把数据集中的题目、作者名剔除出去。这一点很好实现，通过字符串的分割便可以完成。除此之外，由于数据集中存在一些其他信息，如图2.2。  
 ![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/2png.png)
   可以看出，红圈中的内容均为没有价值的数据，由于此类数据很少，再加上要准确的分离出一首诗的内容与这些无用内容是相当困难地，所以我选择若一首诗中出现了符号'_'，'《' ， '[' ，'(' ，'（'，便将它从数据集中删除。这样便有了一个相对规整的数据集，样本总数为252478，之后，将它随机打乱后取80%作为训练集，20%作为测试集。  
   
### 3.	算法
#### 1.	算法
   本次PJ我采用LSTM （LSTM：long short-term memory长短期记忆网络）模型学习生成唐诗。  
   其实LSTM就是忘记以前的文字内容并记忆当前输入的内容，它是对于RNN（递归神经网络）的改进。  
一个典型的RNN模型如图3.1所示，其中X（t）代表输入，h（t）是输出，而A代表从循环中的前一步获得信息的神经网络。一个单元的输出进入下一个单元并且传递信息。  
 ![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/3.png)
                           
   传统的RNN在处理时间序列的问题的效果很好, 但是仍然存在着一些问题, 其中较为严重的是容易出现梯度消失或者梯度爆炸的问题。为了解决这些问题，也就出现了一系列的改进的算法，如LSTM。一方面它有特殊的方式存储”记忆”，因此以前梯度比较大的”记忆”不会像简单的RNN一样马上被抹除，所以可以一定程度上克服梯度消失问题。另一方面，对于梯度爆炸，采取gradient clipping方法，也就是当你计算的梯度超过阈值c或者小于阈值-c的时候，便把此时的梯度设置成c或-c。  
LSTM采用如下结构：  
 ![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/4.png)
								
σ：Sigmoid层，决定记住与忘记信息。  
tanh：tanh层，克服梯度消失问题。  
h（t-1）：上一个LSTM单元的输出。  
c（t-1）：上一个LSTM单元的记忆。  
X（t）：输入。  
c（t）：最新的记忆。  
h（t）：输出。  

#### 2.	具体操作
   首先，LSTM的输入与输出都是矩阵，因此需要有一个机制能将诗歌与矩阵互相转换。为此，需要先统计所有在诗中出现的所有字符（汉字与标点），并统计每个字符的出现次数。以此构造一个字符数组，并将数组元素依出现次数排序。如此构造出来的字符数组是不会包括空格的，之后的操作需要使用空格来统一诗歌的格式，因此在字符数组末尾添加元素空格，并设置其出现次数为-1。为每个字符依据其出现次数排序分配一个id，例如出现次数最多的字符id为1，第二多的字符id为2，以此类推。通过以上操作后，便可以建立两个字典：wordToid，idToword。它们能实现字符与id之间的转换。  
   由于诗歌是字符的集合，所有便可以将诗歌转为相应的矩阵。例如“床前明月光”，假设“床”的id为1，“前”的id为2，“明”的id为3，“月”的id为4，“光”的id为5。那么“床前明月光”便可以转为矩阵[1，2，3，4，5]。  
  将当前的诗歌集转为矩阵作为train_data，由于希望模型能通过当前的字符预测出下一个字符，因此，将train_data向后移一位得到train_label，例如，train_data 为“床前明月光，”，那么train_label便是“前明月光，疑”。  
不过，现在的train_data与train_label还不能直接用于LSTM模型：不同诗歌之间长度（图3.4）有着很大的差别，这样转化的矩阵也并不具有统一的维度，自然也无法应用于LSTM模型。为此，需要对使用空格对短的诗歌进行填充，使得诗歌有相同的长度。  
 ![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/5.png)
   但在这个诗歌集中，大部分诗的长度都比较短，而少部分则非常长。如图3.5，最长的诗长度（含标点）为2988，而诗歌长度平均值却不足60。如果选择令所有诗歌长度与最长诗歌相同，那这样比较短的诗歌里就存在比较多没意义的空格作为补充，这即影响训练结果，又影响训练的速度。  
![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/6.png) 

   一种替代方式便是将诗歌集分批进行处理：首先设置一个batch_size，本次PJ选择64作为batch_size。之后将每batch_size首诗作为一个batch，这样的话就只需要计算每个batch诗词中的最长诗词长度作为batch中的shape列数了，即shape=(batch_size, max_length)，通过这种方式，所需填充的空格便少了很多。  
   在有了train_data与train_label后，所需的便是LSTM模型了。本次PJ中，我选择了tensorflow给的 tf.nn.rnn_cell.BasicLSTMCell生成LSTM基本模型，并使用使用sequence_loss_by_example得到损失函数作为训练目标。上文提到的每个batch的shape=(batch_size, max_length)，不同batch的max_length通常是不同的，所以为了解决这个问题，需要使用tf.nn.dynamic_rnn这个方法。  
   在训练前还需要为模型设置一些超参数，我这里用的是 2层LSTM，LSTM的单元个数为512，batch_size取64，训练 10个 epoch，使用 Adam 优化器进行优化。使用tf.train.exponential_decay函数实现指数衰减学习率：这个函数能保证首先使用较大学习率，从而快速得到一个比较优的解，然后通过迭代逐步减小学习率，从而使模型在训练后期更加稳定。  
   训练结果如图3.6所示：  
 ![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/7.png)
   在自动写诗之前，我们需要定义一个能将矩阵映射到诗句的功能函数，为了避免每次生成的诗都一样，需要引入一定的随机性。不选择输出概率最高的字，而是将概率映射到一个区间上，在区间上随机采样，输出概率大的字对应的区间大，被采样的概率也大，但也有小概率会选择其他字。因为每一个字都有这样的随机性，所以每次作出的诗都完全不一样。  
   用‘[’，‘]’作为诗歌的开始与结束标志。在生成诗歌的时候，定义输入的只有一个字词，然后根据上一个字词推测下一个词的位置。因此，以“[”作为生成诗句时的输入，便能随机生成一首诗。当然，也可以固定每一句话的开头，这样就能生成藏头诗。

   至此，写诗机器人已经搭建完毕，接下来就是实际操作了。  

### 4. 实验结果
   尝试了几种不同的文本生成方式：  
#### 1.	随机开头字符，其他字符自动生成。 结果举例：
五言诗：

	明月直夜口，雨功桃花枝。  
	密冷剥粉体，一灯春雨清。  
	侯派出群束，雨淟云纒群。  
	击明议不在，扫手如羊鹇。  
	悠悠物已燥，告意临川涯。  
	志气九宁尔，阴渎差为为。   
	始功基外异，恐矣消太荣。  
	甃替止若为，大殆平之倾。  
	仁乐固可诧，尧匪久常淋。  
	岂无燿阳人，犹不错飘零。    
	枘乌犬馈饭，凫火走釜餐。  
	遥从耕已高，有机已功荒。  
	爬鹃不瞎语，振卤唯其裁。  
	求中不可卓，世累鸦马焦。  
	我从胡平生，相期奈有今。  
	问之苏偶归，彼投浣靘巢。  
	乾伫本多少，勤苦园中成。  
	冢弗自降型，轻心正莫扬。  
	坐怀陪挽人，引若舞书来。  
	夜深霜露余，空作荆蒿芜。  
  

	贱薄蚕耕陋，欢闻谷浦深。  
	往来无执褐，环挑有邈鲜。  
	为我谢世道，江村无一钱。  
	琛荷须忠报，喜睡发吟烦。  

七言诗：  

	风言幸世寄江乡，空寄银头谪楚雄。  
	柴树风吹烟叶掩，看春细央夕阳侯。  
	春烟感马污檐际，彩上楼前见断惆。  
	世路幽人思无国，买花小径与君携。  

	一过来城照九州，幽迷翠殿几千秋。  
	迎开飞带作仙渡，四斗八华飞驾天。  
	九陌林情新雪艳，御筵秋豁豁香妍。  
	腥芳正说无他态，何事文间写翼公。  


   对于这些自动生成的诗歌，我将从格式、音律、内容三个角度对其进行分析，并以此评判此写诗机器人的性能优劣。  
###### 一．格式
   可以看出，绝大部分情况下，写诗机器人都能写出正确的格式。不过，在少数情况下，它也会犯一些小小的差错：  

	世益坏禅结泚，力塘苏平上林。  
	不朅复孙病，断薄山莱仙。  

   这首诗就有着格式不齐的现象，原因可能是原诗歌集中本身就存在着少量没有拘于格式的诗歌，比如李白的一些作品（《蜀道难》，《梦游天姥吟留别》等），机器也会学到这些信息，所以有时候也会不那么遵守格式。  
   另外，值得注意的是，他偶尔会生成一些《诗经》风格的诗：  

	胼赫天文，惟有神仙。  
	神武是德，神降其中。  


	持戒天地，不然举祀。  
	曷匪于央，乐以为击。  


	协有长峰，来游差乱。  
	广大城夏，双白子暇。  
	大飖三碧，蝗不忘全。  
	挂将鼻孔，此往而莫。  

   我推测是唐诗中应该也有这类风格的诗，被机器学习到了。为了验证我的猜想，我查了一下全唐诗，发现唐诗中确实有四言诗（如：孙思邈《四言诗》），而且在风格上也确实更偏向于《诗经》而不是通常见到的唐诗。  
   注意机器在生成这类四言诗时，用词、意象选择都明显和五言、七言诗不同，可见是已经学到了这样的风格，而不是简单模仿，这是很难得的。  
之前我曾提到过，唐诗大部分长度都不会太长，以四到八句为主，少部分诗歌长度较长，而且长度往往远超一般诗歌的长度。此写诗机器人随机生成的诗歌也反映出了这一事实：大部分诗歌长度为四到八句，长度超过十句诗歌占比估计在10%~20%左右，而且这部分诗歌长度通常会在20句以上。  
总而言之，在格式方面，写诗机器人对于唐诗的模仿已经是相当不错了。  

##### 二．音律
   当然，对于诗歌的要求，格式只是最基本的，除此之位，还需要考虑音律方面的问题。首先，为了令诗文读起来有一种回环往复的音乐感，需要让诗歌押韵。而这个写诗机器人所写出来的诗对于押韵其实做得挺糟糕的，看来押韵对于机器而言是比较复杂的。  
诗词中除了对韵脚的运用有相印的规范外，对于平仄的运用也有一定格式，称为格律。格律准确的诗歌读起来朗朗上口。与写诗机器人在控制韵脚上显示出的无力不同，其对于格律能把握得比较好。  
   以第一首七言诗为例，以下是搜韵网律诗平仄与韵脚校验（https://sou-yun.com/analyzepoem.aspx）的结果：  
![image](https://github.com/PaulDir/Poetry-writing/blob/master/images/8png.png)
   标红处表示平仄有误。标紫红色处表示用韵错误。标绿处表示有一字多音的情况，需要人工校验。可以看出，这首由写诗机器人创作的律诗，韵脚的错误率是相当高的，但在平仄方面，却仅有一处错误，可见其对平仄已经有一定的掌握了。  

#####三．内容
   众所周知，诗歌最重要的是它的内容。此写诗机器人创作的作品，在内容上倒也确实做到了意义连贯，诗歌能初步看出“起-承-转-合”的结构。另外，它还学会了使用一些特别的技巧（例如对偶），例如第一首五言诗中的那一句“往来无执褐，环挑有邈鲜”，就让我联想到了《陋室铭》中那句“谈笑有鸿儒，往来无白丁”。这些技巧往往能使得一首诗增色不少。  
   写诗机器人的作品还有着不同的风格，正如上一段所提到的，写诗机器人是能学到不同的风格，而不是简单模仿，例如第二首五言诗，就带有很明显的田园诗风格。这一点是很难能可贵的。不过，唐朝毕竟是诗歌盛世，各种风格的诗歌百花齐放，所以此写诗机器人的作品风格之间通常也不会那么泾渭分明。  
不过，这些诗歌在整体上内容质量并不算高，它们的整体意义有点模糊，有堆砌词语的嫌疑，虽然明白它讲了什么，却无法理解它想要表达什么（虽然机器确实没有什么想表达的），简单的说，就是这些诗没有灵魂。  

#### 2.	藏头诗
   结果举例：  
   令诗藏头“复旦大学”，  

	复入君恩曾去不，旦令锦锡为送人。  
	大胜祥瑞向三六，学宗百道群因宜。  

   藏头诗的话就显得质量很差了，单就每一句来说还勉强算得上通顺，连在一起就显得不知所云了。推测原因大概是，一首诗的意义和情感表达主要是以两句为单位的，我这里让每一句句首固定，这样强行为还没有完全形成意义单位的诗赋字，大概会导致意义连贯性被破坏。对此好像也没有什么好的解决方案。  
### 5. 结论和感想
   对于这个简单粗暴模型我一开始倒也没有报太大的期望，没想到最终的成果还算不错，写出来的诗也算得上有模有样。虽然还有很多值得改进的地方，但也给了我不少惊喜。
   在诗的格式上，这个模型已经做得相当好了，尽管有少量的格式不齐，也可以采用筛选数据集的方式解决，如想要生成五言诗，就取五言诗作为训练集进行训练，想要生成七言诗，就取七言诗作为训练集进行训练。这样想必能减少格式混乱的几率。  
   音律方面，这个模型能学到平仄，却做不到押韵。我觉得大概是因为在汉语中语韵比较多，而且不同于人，机器对于每个字的发音是没有概念的，对于机器来说，发现韵脚本身就很困难了，更别说做到押韵。但平仄不一样，平仄总共就四种变化，因此，机器能更容易的发现平仄，这一点反倒是与人类恰好相反。  
   内容方面，机器能做到意义连贯，有“起-承-转-合”，会使用对偶等技巧，甚至能实现不同的创作风格，但与诗人的作品相比终究少了些东西。诗人写诗，不管是抱怨不受重用，或是记叙田园生活，抑或是描写自然风光，他们都是在借诗表达自己的内心。但机器是没有所谓思想感情的，所以写出来的作品总是有一种缺少灵魂的感觉。诗歌毕竟是情感的高度凝练，也许它真的不适合用机器进行创作。  
   总而言之，这个模型所表现出来的效果达到了我的期望，仅仅是简单的套用LSTM模型就能达到如此效果，可见神经网络的强大。它能做到的事想必远不止于此，期待能在以后的学习中不断发掘。  
